# Model-Evaluation        ![](https://img.shields.io/badge/Haribaskar-Dhanabalan-brightgreen.svg?colorB=ff0000)

This Package is developed to evaluate the classification | Regression models based on different metrics.

The use case I have solved in this module is to get the actual and predicted value from the users (pred value should be probablity for
classification and continious value for regression). Based on this input it creates a Evaluation table which contains these columns:

**Regression-** `Unique_ModelID`,`Model_Reference_name`,`mean_absolute_error`, `mean_squared_error`, `mean_squared_log_error`,`median_absolute_error`, `r2_score`, `actual_pred_details`,`Time_stamp`.
**Classification-** `Unique_ModelID`,`Model_Reference_name`,`Threshold`,`TP`,`FP`,`FN`,`TN`,`Accuracy`,
`Precision`,`recall`,`f1`,`mcc`,`roc_auc`,`actual_pred_details`,`Time_stamp`.

---
#### Code Structure:
```python
class Evaluation():
  .........
  return(metrics_table)
class evaluation_plots():
  .........
  display plots
class ModelEvaluation(Evaluation,evaluation_plots):
  .........
```

**Evaluation():**
*This Module is used to generate the metrics table for different sets of thresold values for the classification, and
outputs the set of metrics for the regression problem. (which will be later used to plot the graphs and to understand
the model behavior and imporove the model performance)
This Evaluation Class will deal with the classification & Regression models.
This returns the metrics for different set of thresholds*

**evaluation_plots():**
*This class is used to generate the graphs based on the metrics table, which was generated by the above evaluation 
module. These plots can be exported to the local storage for our future reference.*

**ModelEvaluation():**
*This is the main evaluator module - which runs the above two classes and saves the results based on the user request.*

---
## Sample Test Script:

```python
import evaluation_module as eval_mod
import random

## Sample data: (Actual_value/ Predicted_probablity values)
random.seed(9005)
pred_prob = [random.uniform(0,1) for i in range(100) ]
actual = [random.sample(range(0,2),1)[0] for i in range(100)]

# Initializing the module:
evalu = eval_mod.ModelEvaluation(actual = actual, pred = pred_prob)

# Model evaluation call:
metrics_db = evalu.evaluate(evaluate_save=True,plots_show=False)

# Model Comparison call:
evalu.Compare_models(evaluate_db = metrics_db, model_id = [metrics_db['Unique_ModelID'][0]],comparison_metrics = ['Accuracy','mcc'])
```
---
## Contributing

Patches are welcome, preferably as pull requests.

---
### Other Reference for evaluation:
[Documentation here.] - sklearn evaluation (http://edublancas.github.io/sklearn-evaluation)




